---
marp: true
theme: default
class: invert
paginate: true

footer: "Stu Eggerton - September 2025"
html: true

allowlocalfiles: true


---
# GenAI for developers 

---
# ğŸ¤– What is AI?

- **Artificial Intelligence (AI)** = Computers performing tasks that normally need human intelligence  
- **Key abilities:**
  - ğŸ“š Learning & improving  
  - ğŸ§  Reasoning & decision-making  
  - ğŸ—£ï¸ Understanding language  
  - ğŸ‘€ Recognizing patterns/images/sounds  
  - ğŸš— Acting autonomously  ğŸ˜±ğŸ˜±ğŸ˜±

<!-- like giving a toddler the keys to your electric car or trusting your dog with your favourite food-->

---
# ğŸ¤– What is AI NOT?

- ğŸ”® Magic  
- ğŸ”ª A crazy that will replace everything we do

---
# ğŸ¤– How is it going?
- People continue to overestimate AI
- We don't have General Artificial Intelligence Yet
- AI is helping us write documents, generate memes, use energy, generate code
- Hallucinations - a bad word for 

---
# ğŸ¤” Why AI Wonâ€™t Replace Humans (Yet)

- ğŸ¨ **Creativity & Imagination** â†’ AI recombines patterns, humans invent  
- ğŸ§  **Common Sense & Context** â†’ struggles with nuance & ambiguity  
- â¤ï¸ **Emotions & Empathy** â†’ simulates, but doesnâ€™t *feel*  
- âš–ï¸ **Ethics & Values** â†’ no built-in sense of right/wrong  
- ğŸŒ **Real-World Adaptability** â†’ good in narrow tasks, poor in messy situations  

ğŸ‘‰ **Bottom line:**  
AI = fast & scalable  
Humans = creative, ethical, adaptable  
**Together, theyâ€™re stronger.**

---
# ğŸ§© How Does an LLM Work?

- ğŸ“š **Training**: Learns patterns from massive text datasets  
- ğŸ”¢ **Neural Networks**: Billions of parameters adjust to capture language structure  
- ğŸ§® **Tokenization**: Breaks text into tokens (chunks of words/characters)  
- ğŸ² **Prediction**: For each token, predicts the *most likely* next token  
- ğŸ”„ **Iteration**: Repeats prediction â†’ builds sentences, paragraphs, conversations  
- ğŸš€ **Fine-tuning & Alignment**: Trained further to follow instructions, stay safe, and be useful  

ğŸ‘‰ In short:  
LLMs donâ€™t â€œknowâ€ like humans â€” they **predict patterns of language** based on data.  

---
# ğŸ“– What is RAG?

- ğŸ” **Retrieval**: Searches external knowledge (docs, databases, web)  
- ğŸ§© **Augmentation**: Injects retrieved info into the prompt  
- ğŸ¤– **Generation**: LLM uses both context + knowledge to answer  

ğŸ‘‰ **Why it matters:**  
- âœ… Reduces hallucinations  
- âœ… Keeps answers up-to-date  
- âœ… Combines AI reasoning with real data  

**In short:**  
RAG = *Search + AI* â†’ More accurate, grounded answers  

---
# ğŸ—ï¸ Preparing RAG

- ğŸ“š **Collect Knowledge** â†’ domain docs, PDFs, web pages, FAQs  
- âœ‚ï¸ **Chunking** â†’ split into small, retrievable pieces  
- ğŸ”¤ **Embedding** â†’ convert chunks into vector representations  
- ğŸ¦ **Vector Database** â†’ store embeddings for fast semantic search  


---
# ğŸªœ Steps of a RAG query
ğŸ” **Retrieval** â†’ find the most relevant chunks at query time  
â†“
ğŸ§© **Augmentation** â†’ inject retrieved context into the prompt  
â†“
ğŸ¤– ** LLM Generation** â†’ LLM produces grounded Grounded Response âœ… 

<br/>
<br/><br/>
ğŸ‘‰ **Result:** AI that reasons with real data instead of just memory  

---
# Solution on a Page and demo